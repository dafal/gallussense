{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a94918",
   "metadata": {},
   "source": [
    "# üêì GallusSense ‚Äî Train Your Own Rooster Detector!\n",
    "\n",
    "This notebook walks you through training a machine learning model to detect rooster sounds using your own audio data. The final model can be used with the GallusSense application.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb836e8",
   "metadata": {},
   "source": [
    "### 1. Prepare the Dataset with ESC-50 and Custom Samples\n",
    "\n",
    "This step prepares the training and testing dataset for our rooster sound detection model.\n",
    "\n",
    "We do two things here:\n",
    "\n",
    "- **Automatically download and split the ESC-50 dataset**: We use this public dataset as a source for `non_rooster` samples (e.g. urban noise, animals, etc.). We ignore its few rooster samples because they are too limited to train a robust model.\n",
    "- **Encourage you to add your own samples**: You should manually add `.wav` or `.mp3` files of `rooster` sounds (and optionally more `non_rooster` sounds) into these folders:\n",
    "  - `audio/train/rooster`\n",
    "  - `audio/test/rooster`\n",
    "  - `audio/train/non_rooster`\n",
    "  - `audio/test/non_rooster`\n",
    "\n",
    "This hybrid setup helps us train a model that generalizes better to real-world scenarios where rooster audio is collected independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23576c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Define paths\n",
    "download_url = \"https://github.com/karoldvl/ESC-50/archive/master.zip\"\n",
    "zip_path = \"./audio/esc-50.zip\"\n",
    "extract_path = \"./audio\"\n",
    "audio_output_path = Path(\"./audio\")\n",
    "\n",
    "# Download the dataset\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"‚¨áÔ∏è Downloading ESC-50...\")\n",
    "    os.makedirs(\"./audio\", exist_ok=True)\n",
    "    urlretrieve(download_url, zip_path)\n",
    "\n",
    "# Unzip the dataset\n",
    "if not (Path(\"./audio/ESC-50-master\").exists()):\n",
    "    print(\"üì¶ Extracting ESC-50...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./audio\")\n",
    "\n",
    "# Prepare paths\n",
    "esc_root = Path(\"./audio/ESC-50-master\")\n",
    "esc_audio_dir = esc_root / \"audio\"\n",
    "esc_meta_path = esc_root / \"meta/esc50.csv\"\n",
    "\n",
    "# Define output folders\n",
    "train_nonrooster_dir = audio_output_path / \"train/non_rooster\"\n",
    "test_nonrooster_dir = audio_output_path / \"test/non_rooster\"\n",
    "\n",
    "for folder in [train_nonrooster_dir, test_nonrooster_dir]:\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load metadata\n",
    "df = pd.read_csv(esc_meta_path)\n",
    "\n",
    "# Filter out rooster sounds only\n",
    "non_rooster_df = df[df[\"category\"] != \"rooster\"]\n",
    "\n",
    "# Split dataset: 10% in train, 90% in test\n",
    "nonrooster_train, nonrooster_test = train_test_split(non_rooster_df, test_size=0.9, random_state=42)\n",
    "\n",
    "# Copy files\n",
    "def copy_files(rows, src_dir, dest_dir):\n",
    "    for _, row in rows.iterrows():\n",
    "        src = src_dir / row[\"filename\"]\n",
    "        dst = dest_dir / row[\"filename\"]\n",
    "        if src.exists():\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "print(\"üìÅ Copying non-rooster samples...\")\n",
    "copy_files(nonrooster_train, esc_audio_dir, train_nonrooster_dir)\n",
    "copy_files(nonrooster_test, esc_audio_dir, test_nonrooster_dir)\n",
    "\n",
    "print(\"‚úÖ ESC-50 non_rooster dataset is ready and split into train/test.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26392f0",
   "metadata": {},
   "source": [
    "### 2. Generate Metadata for Training Set\n",
    "\n",
    "Now that the dataset is prepared and structured, we generate a metadata CSV file listing all training audio files and their associated labels.\n",
    "\n",
    "This metadata is crucial for processing the dataset programmatically in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c915bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_path = \"audio/train\"\n",
    "categories = [\"rooster\", \"non_rooster\"]\n",
    "audio_extensions = [\".wav\", \".mp3\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(base_path, category)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if any(filename.lower().endswith(ext) for ext in audio_extensions):\n",
    "            data.append({\n",
    "                \"filename\": os.path.join(folder_path, filename),\n",
    "                \"label\": category\n",
    "            })\n",
    "\n",
    "metadata_train = pd.DataFrame(data)\n",
    "metadata_train.to_csv(\"metadata_train_rf.csv\", index=False)\n",
    "metadata_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09666881",
   "metadata": {},
   "source": [
    "### 3. Extract MFCC Audio Features\n",
    "\n",
    "In this step, we extract **MFCCs (Mel-frequency cepstral coefficients)** from the audio files.\n",
    "\n",
    "MFCCs are a compact representation of the audio signal and are commonly used in audio classification tasks because they capture the timbral texture of sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "metadata = pd.read_csv(\"metadata_train_rf.csv\")\n",
    "metadata[\"label_encoded\"] = metadata[\"label\"].map({\"rooster\": 1, \"non_rooster\": 0})\n",
    "\n",
    "X, y = [], []\n",
    "n_mfcc, sr_target = 40, 22050\n",
    "\n",
    "for row in tqdm(metadata.itertuples(), total=len(metadata)):\n",
    "    try:\n",
    "        signal, _ = librosa.load(row.filename, sr=sr_target)\n",
    "        signal = librosa.util.normalize(signal)\n",
    "        mfcc = librosa.feature.mfcc(y=signal, sr=sr_target, n_mfcc=n_mfcc)\n",
    "        X.append(np.mean(mfcc.T, axis=0))\n",
    "        y.append(row.label_encoded)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è {row.filename}: {e}\")\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"‚úÖ MFCC extraction complete\", X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df08e4",
   "metadata": {},
   "source": [
    "### 4. Visualize Class Distribution\n",
    "\n",
    "To better understand the balance of our dataset, we plot the distribution of the different audio classes.\n",
    "\n",
    "This helps ensure that our model won't be biased due to class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dca130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame(X)\n",
    "df[\"label\"] = y\n",
    "sns.countplot(x=\"label\", data=df)\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7170de3",
   "metadata": {},
   "source": [
    "### 5. Train the Random Forest Model\n",
    "\n",
    "We train a **Random Forest classifier**, a robust and interpretable machine learning model, using the MFCC features and the associated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ec6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(class_weight=\"balanced\", n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"‚úÖ Model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6bab46",
   "metadata": {},
   "source": [
    "### 6. Save the Trained Model\n",
    "\n",
    "Once training is complete, we save the trained model to disk so it can later be reused for inference or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4f9c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"gallussense_custom_rf.pkl\")\n",
    "print(\"üíæ Model saved as gallussense_custom_rf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c03e710",
   "metadata": {},
   "source": [
    "### 7. Generate Metadata for Test Set\n",
    "\n",
    "Next, we generate a metadata file for the test set, just like we did for training.\n",
    "\n",
    "This will allow us to run the same MFCC extraction and evaluation pipeline on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ac5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"audio/test\"\n",
    "data = []\n",
    "\n",
    "for category in [\"rooster\", \"non_rooster\"]:\n",
    "    folder_path = os.path.join(base_path, category)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if any(filename.lower().endswith(ext) for ext in [\".wav\", \".mp3\"]):\n",
    "            data.append({\n",
    "                \"filename\": os.path.join(folder_path, filename),\n",
    "                \"label\": category\n",
    "            })\n",
    "\n",
    "metadata_test = pd.DataFrame(data)\n",
    "metadata_test.to_csv(\"metadata_test_rf.csv\", index=False)\n",
    "print(\"‚úÖ Test metadata created!\")\n",
    "metadata_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d22ae5",
   "metadata": {},
   "source": [
    "### 8. Evaluate the Model on the Test Set\n",
    "\n",
    "We evaluate our model on the test data to understand how well it generalizes.\n",
    "\n",
    "The evaluation includes extracting MFCCs from the test set, predicting the labels, and calculating precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b9b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "metadata_test = pd.read_csv(\"metadata_test_rf.csv\")\n",
    "metadata_test[\"label_encoded\"] = metadata_test[\"label\"].map({\"rooster\": 1, \"non_rooster\": 0})\n",
    "\n",
    "X_test, y_test = [], []\n",
    "\n",
    "for row in tqdm(metadata_test.itertuples(), total=len(metadata_test)):\n",
    "    try:\n",
    "        signal, _ = librosa.load(row.filename, sr=sr_target)\n",
    "        signal = librosa.util.normalize(signal)\n",
    "        mfcc = librosa.feature.mfcc(y=signal, sr=sr_target, n_mfcc=n_mfcc)\n",
    "        X_test.append(np.mean(mfcc.T, axis=0))\n",
    "        y_test.append(row.label_encoded)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è {row.filename}: {e}\")\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"non_rooster\", \"rooster\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e1d818",
   "metadata": {},
   "source": [
    "### 9. Visualize the Confusion Matrix\n",
    "\n",
    "We visualize the **confusion matrix** to identify which classes are being confused the most.\n",
    "\n",
    "This is a useful diagnostic to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a334352",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"rooster\", \"non_rooster\"],\n",
    "            yticklabels=[\"rooster\", \"non_rooster\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95730619",
   "metadata": {},
   "source": [
    "### 9. ‚ùå Step 9 ‚Äî Inspect Misclassifications\n",
    "\n",
    "We visualize the **confusion matrix** to identify which classes are being confused the most.\n",
    "\n",
    "This is a useful diagnostic to improve model performance.\n",
    "\n",
    "\n",
    "Finally, we inspect a few specific errors made by the model, including the audio file, the true and predicted labels, and the model's confidence.\n",
    "\n",
    "This helps us understand failure modes and improve the dataset or the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de26bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = model.predict_proba(X_test)\n",
    "errors = np.where(y_test != y_pred)[0]\n",
    "\n",
    "print(f\"‚ùå Number of errors: {len(errors)}\")\n",
    "\n",
    "for idx in errors[:10]:\n",
    "    file = metadata_test.iloc[idx][\"filename\"]\n",
    "    true = \"rooster\" if y_test[idx] == 1 else \"non_rooster\"\n",
    "    pred = \"rooster\" if y_pred[idx] == 1 else \"non_rooster\"\n",
    "    conf = np.max(probas[idx])\n",
    "    print(f\"\\nüîç {file}\\n‚úÖ True: {true}\\n‚ùå Predicted: {pred}\\nüî¢ Confidence: {conf:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
